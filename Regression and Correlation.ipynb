{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from numpy.random import dirichlet\n",
    "#Loaing portoflios, they are .txt files with tickers on each line.\n",
    "df = pd.read_csv('news_data.csv') #Loading the news data, the column title is headline, content is full text\n",
    "os.chdir(\"portfolios\")\n",
    "file_names = os.listdir()\n",
    "file_array = [file for file in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b02ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from numpy.random import dirichlet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Fetch historical returns with error handling for missing tickers\n",
    "def fetch_historical_returns(tickers, start_date, end_date):\n",
    "    valid_tickers = []\n",
    "    all_returns = pd.DataFrame()\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        tickery = ticker.replace('.', '-') #yfinance form\n",
    "        try:\n",
    "            data = yf.download(tickery, start=start_date, end=end_date, progress=False, ignore_tz=True)['Adj Close']\n",
    "            if not data.empty:\n",
    "                returns = data.pct_change().dropna()\n",
    "                all_returns[ticker] = returns\n",
    "                valid_tickers.append(ticker)\n",
    "            else:\n",
    "                pass  # Silently skip tickers with no data\n",
    "        except Exception:\n",
    "            pass  # Silently skip tickers that cause an error\n",
    "    \n",
    "    return all_returns, valid_tickers\n",
    "\n",
    "# Calculate portfolio standard deviation\n",
    "def calculate_portfolio_sd(returns, weights):\n",
    "    portfolio_returns = np.dot(returns, weights)\n",
    "    return np.std(portfolio_returns) * np.sqrt(252)  # Annualized volatility\n",
    "\n",
    "# Calculate individual asset volatilities\n",
    "def calculate_individual_sds(returns):\n",
    "    return returns.std(axis=0) * np.sqrt(252)  # Annualized individual volatilities\n",
    "\n",
    "# Calculate portfolio VaR\n",
    "def calculate_portfolio_var(returns, weights, confidence_level=0.95):\n",
    "    portfolio_returns = np.dot(returns, weights)\n",
    "    return np.percentile(portfolio_returns, (1 - confidence_level) * 100)\n",
    "\n",
    "# Calculate individual asset VaRs\n",
    "def calculate_individual_vars(returns, confidence_level=0.95):\n",
    "    return returns.apply(lambda x: np.percentile(x, (1 - confidence_level) * 100))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Function to fetch news titles from the CSV file within a specified date range\n",
    "def fetch_news_from_csv(ticker, df, start_date=None, end_date=None):\n",
    "    try:\n",
    "        filtered_df = df[df['ticker'] == ticker]\n",
    "        if start_date:\n",
    "            filtered_df = filtered_df[filtered_df['date'] >= start_date]\n",
    "        if end_date:\n",
    "            filtered_df = filtered_df[filtered_df['date'] <= end_date]\n",
    "        return filtered_df['title'].tolist()\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# Function to build a term-frequency matrix for all tickers\n",
    "def build_term_frequency_matrix(headlines_dict):\n",
    "    all_texts = [' '.join(preprocess_text(' '.join(headlines))) for headlines in headlines_dict.values()]\n",
    "    vectorizer = CountVectorizer()\n",
    "    term_freq_matrix = vectorizer.fit_transform(all_texts)\n",
    "    return csr_matrix(term_freq_matrix), vectorizer.get_feature_names_out()\n",
    "\n",
    "# Function to calculate normalized entropy using sparse matrix operations\n",
    "def calculate_lr_matrix(term_freq_matrix, weights):\n",
    "    weighted_matrix = term_freq_matrix.T.dot(weights)\n",
    "    weighted_matrix = np.array(weighted_matrix).flatten()\n",
    "    total_weight = np.sum(weighted_matrix)\n",
    "    probabilities = weighted_matrix / total_weight\n",
    "    H_combined = -np.sum(probabilities * np.log(probabilities + 1e-10))\n",
    "    m = term_freq_matrix.shape[1]\n",
    "    LR = H_combined / np.log(m) if m > 1 else 0\n",
    "    return LR\n",
    "\n",
    "# Generate random portfolios and calculate metrics\n",
    "def generate_portfolios_and_calculate_metrics(tickers, returns, headlines_dict, num_portfolios=1000):\n",
    "    term_freq_matrix, feature_names = build_term_frequency_matrix(headlines_dict)\n",
    "    n = len(tickers)\n",
    "    portfolio_metrics = []\n",
    "\n",
    "    for _ in range(num_portfolios):\n",
    "        weights = np.random.dirichlet(np.ones(n), size=1)[0]\n",
    "        valid_tickers = [tickers[i] for i in range(n) if tickers[i] in headlines_dict and len(headlines_dict[tickers[i]]) > 0]\n",
    "        if not valid_tickers:\n",
    "            continue\n",
    "        valid_weights = [weights[i] for i in range(n) if tickers[i] in valid_tickers]\n",
    "        lr_measure = calculate_lr_matrix(term_freq_matrix, valid_weights)\n",
    "        total_volatility = calculate_portfolio_sd(returns[valid_tickers], valid_weights)\n",
    "        individual_volatilities = calculate_individual_sds(returns[valid_tickers])\n",
    "        total_var = calculate_portfolio_var(returns[valid_tickers], valid_weights)\n",
    "        individual_vars = calculate_individual_vars(returns[valid_tickers])\n",
    "        \n",
    "        if np.any(np.isnan(individual_volatilities)) or np.isnan(total_volatility):\n",
    "            continue\n",
    "\n",
    "        diversification_ratio_vol = total_volatility / np.sum(individual_volatilities * valid_weights)\n",
    "        diversification_ratio_var = total_var / np.sum(individual_vars * valid_weights)\n",
    "        \n",
    "        portfolio_metrics.append((lr_measure, diversification_ratio_vol, diversification_ratio_var))\n",
    "\n",
    "    return portfolio_metrics\n",
    "\n",
    "# Perform linear regression analysis\n",
    "def perform_linear_regression_analysis(lr_measures, diversification_ratios_vol, diversification_ratios_var):\n",
    "    X = np.array(lr_measures).reshape(-1, 1)\n",
    "    y_vol = np.array(diversification_ratios_vol)\n",
    "    y_var = np.array(diversification_ratios_var)\n",
    "\n",
    "    X_train, X_test, y_train_vol, y_test_vol, y_train_var, y_test_var = train_test_split(X, y_vol, y_var, test_size=0.2, random_state=42)\n",
    "\n",
    "    linear_model_vol = LinearRegression()\n",
    "    linear_model_vol.fit(X_train, y_train_vol)\n",
    "    predictions_linear_vol = linear_model_vol.predict(X_test)\n",
    "    r2_linear_vol = r2_score(y_test_vol, predictions_linear_vol)\n",
    "    mse_linear_vol = mean_squared_error(y_test_vol, predictions_linear_vol)\n",
    "\n",
    "    linear_model_var = LinearRegression()\n",
    "    linear_model_var.fit(X_train, y_train_var)\n",
    "    predictions_linear_var = linear_model_var.predict(X_test)\n",
    "    r2_linear_var = r2_score(y_test_var, predictions_linear_var)\n",
    "    mse_linear_var = mean_squared_error(y_test_var, predictions_linear_var)\n",
    "\n",
    "    X_train_sm = sm.add_constant(X_train)  # Add a constant term for the intercept\n",
    "\n",
    "    model_vol = sm.OLS(y_train_vol, X_train_sm).fit()\n",
    "    p_values_vol = model_vol.pvalues\n",
    "    significance_vol = model_vol.summary2().tables[1]['P>|t|'][1]  # p-value of the LR measure for volatility\n",
    "\n",
    "    model_var = sm.OLS(y_train_var, X_train_sm).fit()\n",
    "    p_values_var = model_var.pvalues\n",
    "    significance_var = model_var.summary2().tables[1]['P>|t|'][1]  # p-value of the LR measure for VaR\n",
    "\n",
    "    print(\"\\nLinear Regression (Volatility):\")\n",
    "    print(f\"R² Score: {r2_linear_vol}\")\n",
    "    print(f\"Mean Squared Error: {mse_linear_vol}\")\n",
    "    print(f\"p-value: {significance_vol}\")\n",
    "\n",
    "    print(\"\\nLinear Regression (VaR):\")\n",
    "    print(f\"R² Score: {r2_linear_var}\")\n",
    "    print(f\"Mean Squared Error: {mse_linear_var}\")\n",
    "    print(f\"p-value: {significance_var}\")\n",
    "\n",
    "    return {\n",
    "        'Linear_Vol': (r2_linear_vol, mse_linear_vol, significance_vol),\n",
    "        'Linear_Var': (r2_linear_var, mse_linear_var, significance_var)\n",
    "    }\n",
    "\n",
    "# Function to generate sliding date ranges\n",
    "def generate_date_ranges(start_date, end_date, interval_months):\n",
    "    date_ranges = []\n",
    "    start = start_date\n",
    "    while start < end_date:\n",
    "        end = start + timedelta(days=interval_months * 30)  # Approximate month duration\n",
    "        if end > end_date:\n",
    "            end = end_date\n",
    "        date_ranges.append((start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d')))\n",
    "        start = start + timedelta(days=interval_months * 15)  # Move to next interval (6 months shift)\n",
    "    return date_ranges\n",
    "\n",
    "# Main function with updates\n",
    "def main(file_name, df):\n",
    "    print(\"Processing portfolio:\", file_name)\n",
    "    with open(file_name, 'r') as file:\n",
    "        tickers = [line.strip() for line in file]\n",
    "\n",
    "    start_date = datetime(2018, 1, 1)\n",
    "    end_date = datetime(2024, 6, 30)\n",
    "    interval_months = 12\n",
    "\n",
    "    # Generate sliding windows\n",
    "    date_ranges = generate_date_ranges(start_date, end_date, interval_months)\n",
    "\n",
    "    # Perform analysis for each window\n",
    "    for start_dat, end_dat in date_ranges:\n",
    "        returns, valid_tickers = fetch_historical_returns(tickers, start_dat, end_dat)\n",
    "        if not valid_tickers:\n",
    "            continue  # Skip this period if no valid tickers\n",
    "\n",
    "        headlines_dict = {}\n",
    "        for ticker in valid_tickers:\n",
    "            headlines = fetch_news_from_csv(ticker, df, start_date=start_dat, end_date=end_dat)\n",
    "            if headlines:\n",
    "                headlines_dict[ticker] = headlines\n",
    "\n",
    "        # Only use tickers with valid headlines\n",
    "        valid_tickers_with_headlines = [ticker for ticker in valid_tickers if ticker in headlines_dict]\n",
    "        returns = returns[valid_tickers_with_headlines]\n",
    "\n",
    "        portfolio_metrics = generate_portfolios_and_calculate_metrics(valid_tickers_with_headlines, returns, headlines_dict)\n",
    "        portfolio_metrics = [(lr, drv, drvar) for lr, drv, drvar in portfolio_metrics if not np.isnan(drv) and not np.isnan(drvar)]\n",
    "\n",
    "        if portfolio_metrics:\n",
    "            lr_measures, diversification_ratios_vol, diversification_ratios_var = zip(*portfolio_metrics)\n",
    "        \n",
    "            correlation_vol = np.corrcoef(lr_measures, diversification_ratios_vol)[0, 1]\n",
    "            correlation_var = np.corrcoef(lr_measures, diversification_ratios_var)[0, 1]\n",
    "            print(f\"Correlation between LR Measure and Diversification Ratio (Volatility): {correlation_vol}\")\n",
    "            print(f\"Correlation between LR Measure and Diversification Ratio (VaR): {correlation_var}\")\n",
    "\n",
    "            regression_results = perform_linear_regression_analysis(lr_measures, diversification_ratios_vol, diversification_ratios_var)\n",
    "\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            plt.scatter(lr_measures, diversification_ratios_vol, alpha=0.5, label='Data Points (Volatility)')\n",
    "            plt.xlabel('LR Measure (Normalized Shannon Entropy)')\n",
    "            plt.ylabel('Diversification Ratio (Total Volatility / Sum of Individual Volatilities)')\n",
    "            plt.title(f'Relationship between LR Measure and Diversification Ratio (Volatility)\\nPeriod: {start_dat} to {end_dat}')\n",
    "\n",
    "            linear_model_vol = LinearRegression()\n",
    "            linear_model_vol.fit(np.array(lr_measures).reshape(-1, 1), diversification_ratios_vol)\n",
    "            plt.plot(lr_measures, linear_model_vol.predict(np.array(lr_measures).reshape(-1, 1)), color='red', label=f'Linear Fit (Volatility) (R²={regression_results[\"Linear_Vol\"][0]:.2f})')\n",
    "\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            plt.scatter(lr_measures, diversification_ratios_var, alpha=0.5, label='Data Points (VaR)')\n",
    "            plt.xlabel('LR Measure (Normalized Shannon Entropy)')\n",
    "            plt.ylabel('Diversification Ratio (VaR)')\n",
    "            plt.title(f'Relationship between LR Measure and Diversification Ratio (VaR)\\nPeriod: {start_dat} to {end_dat}')\n",
    "\n",
    "            linear_model_var = LinearRegression()\n",
    "            linear_model_var.fit(np.array(lr_measures).reshape(-1, 1), diversification_ratios_var)\n",
    "            plt.plot(lr_measures, linear_model_var.predict(np.array(lr_measures).reshape(-1, 1)), color='blue', label=f'Linear Fit (VaR) (R²={regression_results[\"Linear_Var\"][0]:.2f})')\n",
    "\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for file_name in file_array:\n",
    "        main(file_name, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e5fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

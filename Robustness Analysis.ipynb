{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from numpy.random import dirichlet\n",
    "#Loaing portoflios, they are .txt files with tickers on each line.\n",
    "df = pd.read_csv('news_data.csv') #Loading the news data, the column title is headline, content is full text\n",
    "os.chdir(\"portfolios\")\n",
    "file_names = os.listdir()\n",
    "file_array = [file for file in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd45eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from numpy.random import dirichlet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import statsmodels.api as sm\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def fetch_historical_returns(tickers, start_date, end_date):\n",
    "    valid_tickers = []\n",
    "    all_returns = pd.DataFrame()\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        tickery = ticker.replace('.', '-') #yfinance form\n",
    "        try:\n",
    "            data = yf.download(tickery, start=start_date, end=end_date, progress=False)['Adj Close']\n",
    "            if not data.empty:\n",
    "                returns = data.pct_change().dropna()\n",
    "                all_returns[ticker] = returns\n",
    "                valid_tickers.append(ticker)\n",
    "            else:\n",
    "                pass\n",
    "        except Exception:\n",
    "            pass  \n",
    "    \n",
    "    return all_returns, valid_tickers\n",
    "\n",
    "# Calculate portfolio standard deviation\n",
    "def calculate_portfolio_sd(returns, weights):\n",
    "    portfolio_returns = np.dot(returns, weights)\n",
    "    return np.std(portfolio_returns) * np.sqrt(252)  # Annualized volatility\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Fetch news headlines from CSV, with date handling\n",
    "def fetch_news_from_csv(ticker, df, start_date=None, end_date=None):\n",
    "    try:\n",
    "        df['date'] = pd.to_datetime(df['date'], utc=True).dt.tz_convert(None)\n",
    "        filtered_df = df[df['ticker'] == ticker]\n",
    "        if start_date:\n",
    "            filtered_df = filtered_df[filtered_df['date'] >= pd.Timestamp(start_date)]\n",
    "        if end_date:\n",
    "            filtered_df = filtered_df[filtered_df['date'] <= pd.Timestamp(end_date)]\n",
    "        if len(filtered_df['title'].tolist()) == 0:\n",
    "            return [\"\"]\n",
    "        return filtered_df['title'].tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to build a term-frequency matrix for all tickers\n",
    "def build_term_frequency_matrix(headlines_dict):\n",
    "    all_texts = [' '.join(preprocess_text(' '.join(headlines))) for headlines in headlines_dict.values()]\n",
    "    vectorizer = CountVectorizer()\n",
    "    term_freq_matrix = vectorizer.fit_transform(all_texts)\n",
    "    return csr_matrix(term_freq_matrix), vectorizer.get_feature_names_out()\n",
    "\n",
    "# Function to calculate normalized entropy using sparse matrix operations\n",
    "def calculate_lr_matrix(term_freq_matrix, weights):\n",
    "    num_documents = term_freq_matrix.shape[0]\n",
    "    if len(weights) != num_documents:\n",
    "        raise ValueError(f\"Dimension mismatch: term frequency matrix has {num_documents} documents but weights vector has {len(weights)} elements.\")\n",
    "    weighted_matrix = term_freq_matrix.T.dot(weights)\n",
    "    weighted_matrix = np.array(weighted_matrix).flatten()\n",
    "    total_weight = np.sum(weighted_matrix)\n",
    "    probabilities = weighted_matrix / total_weight\n",
    "    H_combined = -np.sum(probabilities * np.log(probabilities + 1e-10))\n",
    "    m = term_freq_matrix.shape[1]\n",
    "    LR = H_combined / np.log(m) if m > 1 else 0\n",
    "    return LR\n",
    "\n",
    "# Calculate DR based on Standard Deviation (dr_sd)\n",
    "def calculate_dr_sd(total_volatility, individual_volatilities, weights):\n",
    "    return total_volatility / np.sum(individual_volatilities * weights)\n",
    "\n",
    "# Calculate portfolio VaR (DR based on VaR)\n",
    "def calculate_portfolio_var(returns, weights, confidence_level=0.95):\n",
    "    portfolio_returns = np.dot(returns, weights)\n",
    "    var_value = np.percentile(portfolio_returns, (1 - confidence_level) * 100)\n",
    "    return -var_value\n",
    "\n",
    "# Generate random portfolios and calculate metrics\n",
    "def generate_portfolios_and_calculate_metrics(tickers, returns, headlines_dict, weights):\n",
    "    valid_tickers = [ticker for ticker in tickers if ticker in headlines_dict and len(headlines_dict[ticker]) > 0]\n",
    "    term_freq_matrix, feature_names = build_term_frequency_matrix({ticker: headlines_dict[ticker] for ticker in valid_tickers})\n",
    "    portfolio_metrics = []\n",
    "\n",
    "    for _ in range(1000):\n",
    "        lr_measure = calculate_lr_matrix(term_freq_matrix, weights)\n",
    "        total_volatility = calculate_portfolio_sd(returns[valid_tickers], weights)\n",
    "        individual_volatilities = np.std(returns[valid_tickers], axis=0) * np.sqrt(252)  # Annualized volatilities\n",
    "        dr_sd = calculate_dr_sd(total_volatility, individual_volatilities, weights)\n",
    "        dr_var = calculate_portfolio_var(returns[valid_tickers], weights)\n",
    "        portfolio_metrics.append((lr_measure, total_volatility, dr_sd, dr_var))\n",
    "\n",
    "    return portfolio_metrics\n",
    "\n",
    "# Rolling Window Analysis for Robustness of Each Metric (180-day window, 90-day step)\n",
    "def rolling_window_analysis(tickers, df, window_size=180, step_size=90):\n",
    "    start_date = datetime(2018, 1, 1)\n",
    "    end_date = datetime(2024, 6, 30)\n",
    "    rolling_windows = pd.date_range(start=start_date, end=end_date, freq=f'{step_size}D')\n",
    "    \n",
    "    metrics_summary = {\"lr_std\": [], \"sd_std\": [], \"dr_sd_std\": [], \"dr_var_std\": [],\n",
    "                       \"lr_mean\": [], \"sd_mean\": [], \"dr_sd_mean\": [], \"dr_var_mean\": [],\n",
    "                       \"lr_range\": [], \"sd_range\": [], \"dr_sd_range\": [], \"dr_var_range\": []}\n",
    "\n",
    "    all_lr, all_sd, all_dr_sd, all_dr_var = [], [], [], []\n",
    "\n",
    "    for window_start in rolling_windows:\n",
    "        window_end = window_start + timedelta(days=window_size)\n",
    "        \n",
    "        # Ensure the window doesn't exceed the end_date\n",
    "        if window_end > end_date:\n",
    "            break\n",
    "\n",
    "        # Fetch historical returns for the window\n",
    "        returns,valid = fetch_historical_returns(tickers, window_start, window_end)\n",
    "\n",
    "        if returns.empty:\n",
    "            print(f\"No valid returns data for window: {window_start} to {window_end}\")\n",
    "            continue\n",
    "\n",
    "        # Get news headlines for the window\n",
    "        headlines_dict = {}\n",
    "        for ticker in valid:\n",
    "            headlines = fetch_news_from_csv(ticker, df, start_date=window_start, end_date=window_end)\n",
    "            if headlines:\n",
    "                headlines_dict[ticker] = headlines\n",
    "\n",
    "        # Skip window if no news data\n",
    "        if not headlines_dict:\n",
    "            continue\n",
    "\n",
    "        # Generate random portfolio weights\n",
    "        valid_tickers = [ticker for ticker in valid if ticker in headlines_dict and len(headlines_dict[ticker]) > 0]\n",
    "        if len(valid_tickers) == 0:\n",
    "            continue\n",
    "\n",
    "        n = len(valid_tickers)\n",
    "        weights = np.random.dirichlet(np.ones(n), size=1)[0]\n",
    "\n",
    "        # Calculate portfolio metrics\n",
    "        portfolio_metrics = generate_portfolios_and_calculate_metrics(valid_tickers, returns, headlines_dict, weights)\n",
    "\n",
    "        if portfolio_metrics:\n",
    "            lr_vals, sd_vals, dr_sd_vals, dr_var_vals = zip(*portfolio_metrics)\n",
    "            all_lr.extend(lr_vals)\n",
    "            all_sd.extend(sd_vals)\n",
    "            all_dr_sd.extend(dr_sd_vals)\n",
    "            all_dr_var.extend(dr_var_vals)\n",
    "    \n",
    "    # Calculate robustness measures for each metric\n",
    "    metrics_summary[\"lr_mean\"] = np.mean(all_lr)\n",
    "    metrics_summary[\"sd_mean\"] = np.mean(all_sd)\n",
    "    metrics_summary[\"dr_sd_mean\"] = np.mean(all_dr_sd)\n",
    "    metrics_summary[\"dr_var_mean\"] = np.mean(all_dr_var)\n",
    "\n",
    "    metrics_summary[\"lr_std\"] = np.std(all_lr)\n",
    "    metrics_summary[\"sd_std\"] = np.std(all_sd)\n",
    "    metrics_summary[\"dr_sd_std\"] = np.std(all_dr_sd)\n",
    "    metrics_summary[\"dr_var_std\"] = np.std(all_dr_var)\n",
    "\n",
    "    metrics_summary[\"lr_range\"] = np.max(all_lr) - np.min(all_lr)\n",
    "    metrics_summary[\"sd_range\"] = np.max(all_sd) - np.min(all_sd)\n",
    "    metrics_summary[\"dr_sd_range\"] = np.max(all_dr_sd) - np.min(all_dr_sd)\n",
    "    metrics_summary[\"dr_var_range\"] = np.max(all_dr_var) - np.min(all_dr_var)\n",
    "\n",
    "    metrics_summary[\"lr_cv\"] = metrics_summary[\"lr_std\"] / metrics_summary[\"lr_mean\"]\n",
    "    metrics_summary[\"sd_cv\"] = metrics_summary[\"sd_std\"] / metrics_summary[\"sd_mean\"]\n",
    "    metrics_summary[\"dr_sd_cv\"] = metrics_summary[\"dr_sd_std\"] / metrics_summary[\"dr_sd_mean\"]\n",
    "    metrics_summary[\"dr_var_cv\"] = metrics_summary[\"dr_var_std\"] / metrics_summary[\"dr_var_mean\"]\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Mean for Lexical Ratio (LR): {metrics_summary['lr_mean']:.4f}\")\n",
    "    print(f\"Mean for Standard Deviation (SD): {metrics_summary['sd_mean']:.4f}\")\n",
    "    print(f\"Mean for DR-SD: {metrics_summary['dr_sd_mean']:.4f}\")\n",
    "    print(f\"Mean for DR-Var: {metrics_summary['dr_var_mean']:.4f}\")\n",
    "    \n",
    "    print(f\"Standard Deviation (SD) for Lexical Ratio (LR): {metrics_summary['lr_std']:.4f}\")\n",
    "    print(f\"Standard Deviation (SD) for Standard Deviation (SD): {metrics_summary['sd_std']:.4f}\")\n",
    "    print(f\"Standard Deviation (SD) for DR-SD: {metrics_summary['dr_sd_std']:.4f}\")\n",
    "    print(f\"Standard Deviation (SD) for DR-Var: {metrics_summary['dr_var_std']:.4f}\")\n",
    "    \n",
    "    print(f\"Max-Min Range for Lexical Ratio (LR): {metrics_summary['lr_range']:.4f}\")\n",
    "    print(f\"Max-Min Range for Standard Deviation (SD): {metrics_summary['sd_range']:.4f}\")\n",
    "    print(f\"Max-Min Range for DR-SD: {metrics_summary['dr_sd_range']:.4f}\")\n",
    "    print(f\"Max-Min Range for DR-Var: {metrics_summary['dr_var_range']:.4f}\")\n",
    "    \n",
    "    print(f\"Coefficient of Variation (CV) for Lexical Ratio (LR): {metrics_summary['lr_cv']:.4f}\")\n",
    "    print(f\"Coefficient of Variation (CV) for Standard Deviation (SD): {metrics_summary['sd_cv']:.4f}\")\n",
    "    print(f\"Coefficient of Variation (CV) for DR-SD: {metrics_summary['dr_sd_cv']:.4f}\")\n",
    "    print(f\"Coefficient of Variation (CV) for DR-Var: {metrics_summary['dr_var_cv']:.4f}\")\n",
    "\n",
    "# Main function to run the robustness analysis\n",
    "def main():\n",
    "    for file_name in file_array:\n",
    "        print(f\"Processing portfolio: {file_name}\")\n",
    "        with open(file_name, 'r') as file:\n",
    "            tickers = [line.strip() for line in file]\n",
    "        rolling_window_analysis(tickers, df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c02bdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e828aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from numpy.random import dirichlet\n",
    "#Loaing portoflios, they are .txt files with tickers on each line.\n",
    "df = pd.read_csv('news_data.csv') #Loading the news data, the column title is headline, content is full text\n",
    "os.chdir(\"portfolios\")\n",
    "file_names = os.listdir()\n",
    "file_array = [file for file in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2235652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from scipy.optimize import minimize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import string\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Options for optimization\n",
    "options = {\n",
    "    'maxiter': 3000,\n",
    "    'ftol': 1e-7,\n",
    "    'gtol': 1e-7,\n",
    "    'disp': False,\n",
    "    'eps': 1e-7\n",
    "}\n",
    "\n",
    "# Preprocess text (Lemmatization, stopword removal, etc.)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Fetch news titles from CSV for a specified date range\n",
    "def fetch_news_from_csv(ticker, start_date=None, end_date=None):\n",
    "    try:\n",
    "        filtered_df = df[df['ticker'] == ticker]\n",
    "        if start_date:\n",
    "            filtered_df = filtered_df[filtered_df['date'] >= start_date]\n",
    "        if end_date:\n",
    "            filtered_df = filtered_df[filtered_df['date'] <= end_date]\n",
    "        return filtered_df['title'].tolist()\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# Build a term-frequency matrix for all tickers\n",
    "def build_term_frequency_matrix(headlines_dict):\n",
    "    all_texts = [' '.join(preprocess_text(' '.join(headlines))) for headlines in headlines_dict.values()]\n",
    "    vectorizer = CountVectorizer()\n",
    "    term_freq_matrix = vectorizer.fit_transform(all_texts)\n",
    "    return csr_matrix(term_freq_matrix), vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate Lexical Ratio using term frequency matrix\n",
    "def calculate_lr_matrix(term_freq_matrix, weights):\n",
    "    weighted_matrix = term_freq_matrix.T.dot(weights)\n",
    "    weighted_matrix = np.array(weighted_matrix).flatten()\n",
    "    total_weight = np.sum(weighted_matrix)\n",
    "    probabilities = weighted_matrix / total_weight\n",
    "    H_combined = -np.sum(probabilities * np.log(probabilities + 1e-10))\n",
    "    m = term_freq_matrix.shape[1]\n",
    "    LR = H_combined / np.log(m) if m > 1 else 0\n",
    "    return LR\n",
    "\n",
    "# Calculate weighted return percentage\n",
    "def calculate_weighted_return_percentage(returns, weights):\n",
    "    return np.sum(returns.mean() * weights) * 252\n",
    "\n",
    "# Function to calculate key metrics\n",
    "def calculate_metrics(weights, returns, risk_free_rate=0.024):\n",
    "    portfolio_returns = np.dot(returns, weights)\n",
    "    portfolio_return = np.sum(returns.mean() * weights) * 252\n",
    "    portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * 252, weights)))\n",
    "    excess_return = portfolio_return - risk_free_rate\n",
    "    sharpe_ratio = excess_return / portfolio_volatility\n",
    "    downside_returns = np.where(portfolio_returns < 0, portfolio_returns, 0)\n",
    "    downside_volatility = np.std(downside_returns) * np.sqrt(252)\n",
    "    sortino_ratio = excess_return / downside_volatility if downside_volatility != 0 else np.nan\n",
    "    return {\n",
    "        \"Sharpe Ratio\": sharpe_ratio,\n",
    "        \"Sortino Ratio\": sortino_ratio,\n",
    "        \"Annualized Return\": round(portfolio_return, 4),\n",
    "        \"Annualized Volatility\": round(portfolio_volatility, 4),\n",
    "        \"Downside Volatility\": round(downside_volatility, 4)\n",
    "    }\n",
    "# Build a term-frequency matrix for all tickers\n",
    "def build_term_frequency_matrix(headlines_dict):\n",
    "    all_texts = [' '.join(preprocess_text(' '.join(headlines))) for headlines in headlines_dict.values()]\n",
    "    vectorizer = CountVectorizer()\n",
    "    term_freq_matrix = vectorizer.fit_transform(all_texts)\n",
    "    return csr_matrix(term_freq_matrix), vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate Lexical Ratio using term frequency matrix\n",
    "def calculate_lr_matrix(term_freq_matrix, weights):\n",
    "    weighted_matrix = term_freq_matrix.T.dot(weights)\n",
    "    weighted_matrix = np.array(weighted_matrix).flatten()\n",
    "    total_weight = np.sum(weighted_matrix)\n",
    "    probabilities = weighted_matrix / total_weight\n",
    "    H_combined = -np.sum(probabilities * np.log(probabilities + 1e-10))\n",
    "    m = term_freq_matrix.shape[1]\n",
    "    LR = H_combined / np.log(m) if m > 1 else 0\n",
    "    return LR\n",
    "\n",
    "# Optimization using Lexical Ratio (LR)\n",
    "def optimize_portfolio_lr(tickers, returns, target_returns, headlines_dict):\n",
    "    term_freq_matrix, feature_names = build_term_frequency_matrix(headlines_dict)\n",
    "    n = len(tickers)\n",
    "    results = []\n",
    "\n",
    "    for target_return in target_returns:\n",
    "        def objective(weights):\n",
    "            lr = calculate_lr_matrix(term_freq_matrix, weights)\n",
    "            return -lr  # Maximize LR by minimizing negative LR\n",
    "\n",
    "        constraints = [\n",
    "            {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1},\n",
    "            {'type': 'ineq', 'fun': lambda weights: calculate_weighted_return_percentage(returns, weights) - target_return}\n",
    "        ]\n",
    "        bounds = tuple((0, 1) for _ in range(n))\n",
    "        initial_weights = np.ones(n) / n\n",
    "        result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, options=options, constraints=constraints)\n",
    "        if result.success:\n",
    "            results.append((target_return, result.x))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Markowitz Optimization\n",
    "def traditional_markowitz(tickers, returns, target_returns):\n",
    "    n = len(tickers)\n",
    "    results = []\n",
    "\n",
    "    for target_return in target_returns:\n",
    "        def objective(weights):\n",
    "            portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * 252, weights)))\n",
    "            return portfolio_volatility\n",
    "\n",
    "        constraints = [\n",
    "            {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1},\n",
    "            {'type': 'ineq', 'fun': lambda weights: np.sum(returns.mean() * weights) * 252 - target_return}\n",
    "        ]\n",
    "        bounds = tuple((0, 1) for _ in range(n))\n",
    "        initial_weights = np.ones(n) / n\n",
    "        result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, options=options, constraints=constraints)\n",
    "        if result.success:\n",
    "            results.append((target_return, result.x))\n",
    "\n",
    "    return results\n",
    "\n",
    "# DR-SD Optimization\n",
    "def optimize_portfolio_dr_sd(tickers, returns, target_returns):\n",
    "    n = len(tickers)\n",
    "    results = []\n",
    "\n",
    "    for target_return in target_returns:\n",
    "        def objective(weights):\n",
    "            portfolio_sd = np.std(np.dot(returns, weights)) * np.sqrt(252)\n",
    "            individual_sds = np.std(returns, axis=0) * np.sqrt(252)\n",
    "            dr_sd = portfolio_sd / np.mean(individual_sds)\n",
    "            return dr_sd\n",
    "\n",
    "        constraints = [\n",
    "            {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1},\n",
    "            {'type': 'ineq', 'fun': lambda weights: np.sum(returns.mean() * weights) * 252 - target_return}\n",
    "        ]\n",
    "        bounds = tuple((0, 1) for _ in range(n))\n",
    "        initial_weights = np.ones(n) / n\n",
    "\n",
    "        result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, options=options, constraints=constraints)\n",
    "        if result.success:\n",
    "            results.append((target_return, result.x))\n",
    "\n",
    "    return results\n",
    "\n",
    "# DR-VaR Optimization\n",
    "def optimize_portfolio_dr_var(tickers, returns, target_returns, alpha=0.05):\n",
    "    n = len(tickers)\n",
    "    results = []\n",
    "\n",
    "    def value_at_risk(returns, alpha=alpha):\n",
    "        return np.percentile(returns, 100 * alpha)\n",
    "\n",
    "    for target_return in target_returns:\n",
    "        def objective(weights):\n",
    "            portfolio_returns = np.dot(returns, weights)\n",
    "            portfolio_var = value_at_risk(portfolio_returns) * np.sqrt(252)\n",
    "            individual_vars = np.array([value_at_risk(returns.iloc[:, i]) for i in range(returns.shape[1])])\n",
    "            dr_var = portfolio_var / np.mean(individual_vars)\n",
    "            return dr_var\n",
    "\n",
    "        constraints = [\n",
    "            {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1},\n",
    "            {'type': 'ineq', 'fun': lambda weights: np.sum(returns.mean() * weights) * 252 - target_return}\n",
    "        ]\n",
    "        bounds = tuple((0, 1) for _ in range(n))\n",
    "        initial_weights = np.ones(n) / n\n",
    "\n",
    "        result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, options=options, constraints=constraints)\n",
    "        if result.success:\n",
    "            results.append((target_return, result.x))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Fetch historical returns for tickers with error handling\n",
    "def fetch_historical_returns(tickers, start_date, end_date):\n",
    "    valid_tickers = []\n",
    "    all_returns = pd.DataFrame()\n",
    "    for ticker in tickers:\n",
    "        tickery=ticker.replace(\".\",\"-\") #yfinance form\n",
    "        try:\n",
    "            data = yf.download(tickery, start=start_date, end=end_date, progress=False)['Adj Close']\n",
    "            if not data.empty:\n",
    "                returns = data.pct_change().dropna()\n",
    "                all_returns[ticker] = returns\n",
    "                valid_tickers.append(ticker)\n",
    "        except Exception:\n",
    "            pass \n",
    "    \n",
    "    return all_returns, valid_tickers\n",
    "\n",
    "# Generate date ranges for optimization and testing\n",
    "def generate_date_ranges(start_date, end_date, opt_period_months, test_period_months):\n",
    "    date_ranges = []\n",
    "    start = start_date\n",
    "    while start < end_date:\n",
    "        opt_end = start + timedelta(days=opt_period_months * 30)\n",
    "        test_start = opt_end\n",
    "        test_end = test_start + timedelta(days=test_period_months * 30)\n",
    "\n",
    "        if test_end > end_date:\n",
    "            break\n",
    "\n",
    "        date_ranges.append(((start.strftime('%Y-%m-%d'), opt_end.strftime('%Y-%m-%d')),\n",
    "                            (test_start.strftime('%Y-%m-%d'), test_end.strftime('%Y-%m-%d'))))\n",
    "        \n",
    "        start = start + timedelta(days=3 * 30)\n",
    "\n",
    "    return date_ranges\n",
    "\n",
    "# Function to plot results\n",
    "def plot(portfolio_files, start_date='2018-01-01', end_date='2024-06-30', opt_period_months=24, test_period_months=6, target_returns=[0.7,0.1,0.13,0.16], risk_free_rate=0.024):\n",
    "    for file_name in portfolio_files:\n",
    "        print(f\"\\nProcessing portfolio: {file_name}\\n\")\n",
    "        \n",
    "        # Load portfolio tickers\n",
    "        with open(file_name, 'r') as file:\n",
    "            tickers = [line.strip() for line in file]\n",
    "\n",
    "        if not tickers:\n",
    "            print(f\"No tickers found in file: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        # Generate date ranges\n",
    "        date_ranges = generate_date_ranges(datetime.strptime(start_date, '%Y-%m-%d'), datetime.strptime(end_date, '%Y-%m-%d'), opt_period_months, test_period_months)\n",
    "\n",
    "        # Initialize dictionaries to store results\n",
    "        sortino_values = {method: [] for method in ['Lexical Ratio', 'Volatility (Markowitz)', 'Diversification Ratio based on Standard Deviation', 'Diversification Ratio based on Value at Risk']}\n",
    "        sharpe_values = {method: [] for method in ['Lexical Ratio', 'Volatility (Markowitz)', 'Diversification Ratio based on Standard Deviation', 'Diversification Ratio based on Value at Risk']}\n",
    "        downside_vol_values = {method: [] for method in ['Lexical Ratio', 'Volatility (Markowitz)', 'Diversification Ratio based on Standard Deviation', 'Diversification Ratio based on Value at Risk']}\n",
    "        annual_vol_values = {method: [] for method in ['Lexical Ratio', 'Volatility (Markowitz)', 'Diversification Ratio based on Standard Deviation', 'Diversification Ratio based on Value at Risk']}\n",
    "        annual_return_values = {method: [] for method in ['Lexical Ratio', 'Volatility (Markowitz)', 'Diversification Ratio based on Standard Deviation', 'Diversification Ratio based on Value at Risk']}\n",
    "        date_labels = []\n",
    "\n",
    "        for opt_window, test_window in date_ranges:\n",
    "            window_start, window_end = opt_window\n",
    "            test_start, test_end = test_window\n",
    "            \n",
    "            print(f\"\\nOptimizing on window: {window_start} to {window_end}\\nTesting on window: {test_start} to {test_end}\\n\")\n",
    "            \n",
    "            # Fetch historical returns for the optimization window\n",
    "            returns, valid_tickers = fetch_historical_returns(tickers, window_start, window_end)\n",
    "            if not valid_tickers:\n",
    "                print(f\"No valid tickers found for optimization in file: {file_name} for window: {window_start} to {window_end}\")\n",
    "                continue\n",
    "\n",
    "            # Fetch and store news headlines for each stock in the optimization window\n",
    "            headlines_dict = {}\n",
    "            for ticker in valid_tickers:\n",
    "                headlines = fetch_news_from_csv(ticker, start_date=window_start, end_date=window_end)\n",
    "                headlines_dict[ticker] = headlines\n",
    "\n",
    "            # Optimize portfolios for the current window\n",
    "            lr_results = optimize_portfolio_lr(valid_tickers, returns, target_returns, headlines_dict)\n",
    "            markowitz_results = traditional_markowitz(valid_tickers, returns, target_returns=target_returns)\n",
    "            dr_sd_results = optimize_portfolio_dr_sd(valid_tickers, returns, target_returns)\n",
    "            dr_var_results = optimize_portfolio_dr_var(valid_tickers, returns, target_returns)\n",
    "\n",
    "            # Fetch historical returns for the testing window\n",
    "            test_returns, valid_tickers_test = fetch_historical_returns(valid_tickers, test_start, test_end)\n",
    "            if not valid_tickers_test:\n",
    "                print(f\"No valid tickers for testing window: {test_start} to {test_end}\")\n",
    "                continue\n",
    "\n",
    "            # Calculate and collect performance metrics for the testing window\n",
    "            try:\n",
    "                lr_weights = lr_results[0][1]  # First target return for plotting\n",
    "                lr_metrics = calculate_metrics(lr_weights, test_returns, risk_free_rate)\n",
    "                markowitz_weights = markowitz_results[0][1]\n",
    "                markowitz_metrics = calculate_metrics(markowitz_weights, test_returns, risk_free_rate)\n",
    "                dr_sd_weights = dr_sd_results[0][1]\n",
    "                dr_sd_metrics = calculate_metrics(dr_sd_weights, test_returns, risk_free_rate)\n",
    "                dr_var_weights = dr_var_results[0][1]\n",
    "                dr_var_metrics = calculate_metrics(dr_var_weights, test_returns, risk_free_rate)\n",
    "            except IndexError as e:\n",
    "                print(f\"Optimization results missing for some methods: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Collect the metrics for plotting\n",
    "            sortino_values['Lexical Ratio'].append(lr_metrics['Sortino Ratio'])\n",
    "            sortino_values['Volatility (Markowitz)'].append(markowitz_metrics['Sortino Ratio'])\n",
    "            sortino_values['Diversification Ratio based on Standard Deviation'].append(dr_sd_metrics['Sortino Ratio'])\n",
    "            sortino_values['Diversification Ratio based on Value at Risk'].append(dr_var_metrics['Sortino Ratio'])\n",
    "\n",
    "            sharpe_values['Lexical Ratio'].append(lr_metrics['Sharpe Ratio'])\n",
    "            sharpe_values['Volatility (Markowitz)'].append(markowitz_metrics['Sharpe Ratio'])\n",
    "            sharpe_values['Diversification Ratio based on Standard Deviation'].append(dr_sd_metrics['Sharpe Ratio'])\n",
    "            sharpe_values['Diversification Ratio based on Value at Risk'].append(dr_var_metrics['Sharpe Ratio'])\n",
    "\n",
    "\n",
    "            downside_vol_values['Lexical Ratio'].append(lr_metrics['Downside Volatility'])\n",
    "            downside_vol_values['Volatility (Markowitz)'].append(markowitz_metrics['Downside Volatility'])\n",
    "            downside_vol_values['Diversification Ratio based on Standard Deviation'].append(dr_sd_metrics['Downside Volatility'])\n",
    "            downside_vol_values['Diversification Ratio based on Value at Risk'].append(dr_var_metrics['Downside Volatility'])\n",
    "\n",
    "            annual_vol_values['Lexical Ratio'].append(lr_metrics['Annualized Volatility'])\n",
    "            annual_vol_values['Volatility (Markowitz)'].append(markowitz_metrics['Annualized Volatility'])\n",
    "            annual_vol_values['Diversification Ratio based on Standard Deviation'].append(dr_sd_metrics['Annualized Volatility'])\n",
    "            annual_vol_values['Diversification Ratio based on Value at Risk'].append(dr_var_metrics['Annualized Volatility'])\n",
    "\n",
    "            annual_return_values['Lexical Ratio'].append(lr_metrics['Annualized Return'])\n",
    "            annual_return_values['Volatility (Markowitz)'].append(markowitz_metrics['Annualized Return'])\n",
    "            annual_return_values['Diversification Ratio based on Standard Deviation'].append(dr_sd_metrics['Annualized Return'])\n",
    "            annual_return_values['Diversification Ratio based on Value at Risk'].append(dr_var_metrics['Annualized Return'])\n",
    "\n",
    "            # Store the date label for the x-axis\n",
    "            date_labels.append(f\"{test_start} to {test_end}\")\n",
    "\n",
    "        # Define line styles and markers\n",
    "        line_styles = {\n",
    "            'Lexical Ratio': ('-', 'o'),  # Solid line with circles\n",
    "            'Volatility (Markowitz)': ('--', 's'),  # Dashed line with squares\n",
    "            'Diversification Ratio based on Standard Deviation': (':', '^'),  # Dotted line with triangles\n",
    "            'Diversification Ratio based on Value at Risk': ('-.', 'D')  # Dash-dot line with diamonds\n",
    "        }\n",
    "\n",
    "        # Plot Sortino Ratio graph\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for method, values in sortino_values.items():\n",
    "            linestyle, marker = line_styles[method]\n",
    "            plt.plot(date_labels, values, label=method, linestyle=linestyle, marker=marker, markersize=8)\n",
    "        plt.title(f'Sortino Ratio - {file_name.replace(\".txt\", \"\")}')\n",
    "        plt.xlabel('Date Interval Tested On')\n",
    "        plt.ylabel('Sortino Ratio')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Sharpe Ratio graph\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for method, values in sharpe_values.items():\n",
    "            linestyle, marker = line_styles[method]\n",
    "            plt.plot(date_labels, values, label=method, linestyle=linestyle, marker=marker, markersize=8)\n",
    "        plt.title(f'Sharpe Ratio - {file_name.replace(\".txt\", \"\")}')\n",
    "        plt.xlabel('Date Interval Tested On')\n",
    "        plt.ylabel('Sharpe Ratio')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Downside Volatility graph\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for method, values in downside_vol_values.items():\n",
    "            linestyle, marker = line_styles[method]\n",
    "            plt.plot(date_labels, values, label=method, linestyle=linestyle, marker=marker, markersize=8)\n",
    "        plt.title(f'Downside Volatility - {file_name.replace(\".txt\", \"\")}')\n",
    "        plt.xlabel('Date Interval Tested On')\n",
    "        plt.ylabel('Downside Volatility')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Annualized Volatility graph\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for method, values in annual_vol_values.items():\n",
    "            linestyle, marker = line_styles[method]\n",
    "            plt.plot(date_labels, values, label=method, linestyle=linestyle, marker=marker, markersize=8)\n",
    "        plt.title(f'Annualized Volatility - {file_name.replace(\".txt\", \"\")}')\n",
    "        plt.xlabel('Date Interval Tested On')\n",
    "        plt.ylabel('Annualized Volatility')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Annualized Return graph\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for method, values in annual_return_values.items():\n",
    "            linestyle, marker = line_styles[method]\n",
    "            plt.plot(date_labels, values, label=method, linestyle=linestyle, marker=marker, markersize=8)\n",
    "        plt.title(f'Annualized Return - {file_name.replace(\".txt\", \"\")}')\n",
    "        plt.xlabel('Date Interval Tested On')\n",
    "        plt.ylabel('Annualized Return')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504e3818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ranking and values\n",
    "def ranks(portfolio_files, start_date='2018-01-01', end_date='2024-06-30', opt_period_months=24, test_period_months=6, target_returns=[0.07, 0.1, 0.13, 0.16], risk_free_rate=0.024):\n",
    "    \n",
    "    # Initialize a dictionary to store results for each portfolio\n",
    "    all_portfolio_metrics = {file_name: {'lr': [], 'markowitz': [], 'dr_sd': [], 'dr_var': []} for file_name in portfolio_files}\n",
    "    \n",
    "    for file_name in portfolio_files:\n",
    "        print(f\"\\nProcessing portfolio: {file_name}\\n\")\n",
    "        \n",
    "        # Load portfolio tickers\n",
    "        with open(file_name, 'r') as file:\n",
    "            tickers = [line.strip() for line in file]\n",
    "\n",
    "        if not tickers:\n",
    "            print(f\"No tickers found in file: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        # Generate sliding date ranges\n",
    "        date_ranges = generate_date_ranges(datetime.strptime(start_date, '%Y-%m-%d'), datetime.strptime(end_date, '%Y-%m-%d'), opt_period_months, test_period_months)\n",
    "\n",
    "        # Initialize a dictionary to accumulate metrics across all intervals\n",
    "        metrics_accumulator = {method: [] for method in all_portfolio_metrics[file_name].keys()}\n",
    "        \n",
    "        for opt_window, test_window in date_ranges:\n",
    "            window_start, window_end = opt_window\n",
    "            test_start, test_end = test_window\n",
    "            \n",
    "            print(f\"\\nOptimizing on window: {window_start} to {window_end}\\nTesting on window: {test_start} to {test_end}\\n\")\n",
    "            \n",
    "            # Fetch historical returns for the optimization window\n",
    "            returns , valid_tick = fetch_historical_returns(tickers, window_start, window_end)\n",
    "            returns_test , valid_tickers = fetch_historical_returns(tickers, test_start, test_end)\n",
    "            # Fetch and store news headlines for each stock in the optimization window\n",
    "            headlines_dict = {}\n",
    "            for ticker in valid_tickers:\n",
    "                headlines = fetch_news_from_csv(ticker, start_date=window_start, end_date=window_end)\n",
    "                headlines_dict[ticker] = headlines\n",
    "            if not any(headlines_dict.values()):\n",
    "                print(f\"No news fetched for any tickers in file: {file_name} for window: {window_start} to {window_end}\")\n",
    "                continue\n",
    "\n",
    "            # Optimize portfolios for the current window\n",
    "            lr_results = optimize_portfolio_lr(valid_tickers, returns, target_returns, headlines_dict)\n",
    "            markowitz_results = traditional_markowitz(valid_tickers, returns, target_returns)\n",
    "            dr_sd_results = optimize_portfolio_dr_sd(valid_tickers, returns, target_returns)\n",
    "            dr_var_results = optimize_portfolio_dr_var(valid_tickers, returns, target_returns)\n",
    "\n",
    "            # Fetch historical returns for the testing window\n",
    "            test_returns, valid_tickers_test = fetch_historical_returns(valid_tickers , test_start, test_end)\n",
    "            # Calculate performance metrics for the testing window across all target returns\n",
    "            for i in range(len(target_returns)):\n",
    "                try:\n",
    "                    lr_weights = lr_results[i][1]\n",
    "                    lr_metrics = calculate_metrics(lr_weights, test_returns, risk_free_rate)\n",
    "                    markowitz_weights = markowitz_results[i][1]\n",
    "                    markowitz_metrics = calculate_metrics(markowitz_weights, test_returns, risk_free_rate)\n",
    "                    dr_sd_weights = dr_sd_results[i][1]\n",
    "                    dr_sd_metrics = calculate_metrics(dr_sd_weights, test_returns, risk_free_rate)\n",
    "                    dr_var_weights = dr_var_results[i][1]\n",
    "                    dr_var_metrics = calculate_metrics(dr_var_weights, test_returns, risk_free_rate)\n",
    "                    \n",
    "                    # Accumulate metrics for each method across intervals and returns\n",
    "                    metrics_accumulator['lr'].append(lr_metrics)\n",
    "                    metrics_accumulator['markowitz'].append(markowitz_metrics)\n",
    "                    metrics_accumulator['dr_sd'].append(dr_sd_metrics)\n",
    "                    metrics_accumulator['dr_var'].append(dr_var_metrics)\n",
    "                    \n",
    "                except IndexError as e:\n",
    "                    continue\n",
    "\n",
    "        # After all intervals for the current portfolio are processed, calculate the mean metrics over all intervals and target returns\n",
    "        mean_metrics = {method: {} for method in metrics_accumulator.keys()}\n",
    "        for method, metrics_list in metrics_accumulator.items():\n",
    "            if metrics_list:\n",
    "                for metric in metrics_list[0].keys():\n",
    "                    # Compute the mean of all intervals and target returns\n",
    "                    mean_metrics[method][metric] = np.mean([metrics[metric] for metrics in metrics_list])\n",
    "\n",
    "        # Print mean metrics for the current portfolio\n",
    "        print(f\"\\nMean Metrics over all intervals for Portfolio: {file_name}\")\n",
    "        for method, metrics in mean_metrics.items():\n",
    "            print(f\"\\nMethod: {method.capitalize()}\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        # Ranking based on metrics\n",
    "        print(f\"\\nRanking for Portfolio: {file_name}\")\n",
    "        ranking = {metric: sorted(mean_metrics.keys(), key=lambda x: mean_metrics[x][metric], reverse=(metric not in [\"Annualized Volatility\", \"Downside Volatility\"])) for metric in mean_metrics['lr'].keys()}\n",
    "        \n",
    "        print(f\"\\nRanking of Methods for Portfolio: {file_name}\")\n",
    "        for metric, methods in ranking.items():\n",
    "            print(f\"\\nMetric: {metric}\")\n",
    "            for rank, method in enumerate(methods, 1):\n",
    "                print(f\"{rank}. {method.capitalize()} with {metric}: {mean_metrics[method][metric]:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae55663",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "portfolio_files = file_array\n",
    "ranks(portfolio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e876b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(portfolio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899fb57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a96f06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782804cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

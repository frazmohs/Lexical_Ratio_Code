{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f49b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from numpy.random import dirichlet\n",
    "#Loaing portoflios, they are .txt files with tickers on each line.\n",
    "df = pd.read_csv('news_data.csv') #Loading the news data, the column title is headline, content is full text\n",
    "os.chdir(\"portfolios\")\n",
    "file_names = os.listdir()\n",
    "file_array = [file for file in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291016a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import yfinance as yf\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import nltk\n",
    "from decimal import Decimal, getcontext\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "getcontext().prec = 50\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def normalize(series):\n",
    "    return (series - series.min()) / (series.max() - series.min())\n",
    "\n",
    "# Function to fetch news titles from the CSV file within a specified date range\n",
    "def fetch_news_from_csv(ticker, df, start_date=None, end_date=None):\n",
    "    try:\n",
    "        filtered_df = df[df['ticker'] == ticker]\n",
    "        if start_date:\n",
    "            filtered_df = filtered_df[filtered_df['date'] >= start_date]\n",
    "        if end_date:\n",
    "            filtered_df = filtered_df[filtered_df['date'] <= end_date]\n",
    "        # Using the titles here, could be modified to content or both\n",
    "        return filtered_df['title'].tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_date_ranges(start_date, end_date, interval_months):\n",
    "    date_ranges = []\n",
    "    start = start_date\n",
    "    while start < end_date:\n",
    "        end = start + timedelta(days=interval_months * 30)  # Approximate month duration\n",
    "        if end > end_date:\n",
    "            end = end_date\n",
    "        date_ranges.append((start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d')))\n",
    "        start = start + timedelta(days=180)  # Move to next interval\n",
    "    return date_ranges\n",
    "\n",
    "# Fetch historical returns with error handling for missing tickers\n",
    "def fetch_historical_returns(tickers, start_date, end_date):\n",
    "    valid_tickers = []\n",
    "    all_returns = pd.DataFrame()\n",
    "    for ticker in tickers:\n",
    "        tickery = ticker.replace('.', '-') #yfinance form\n",
    "        try:\n",
    "            data = yf.download(tickery, start=start_date, end=end_date, progress=False)['Adj Close']\n",
    "            if not data.empty:\n",
    "                returns = data.pct_change().dropna()\n",
    "                all_returns[ticker] = returns\n",
    "                valid_tickers.append(ticker)\n",
    "            else:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    return all_returns, valid_tickers\n",
    "\n",
    "# Fetch and normalize VIX data\n",
    "def fetch_vix_data(start_date, end_date):\n",
    "    vix_data = yf.download('^VIX', start=start_date, end=end_date, progress=False)['Adj Close']\n",
    "    return normalize(vix_data).dropna()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize the words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Function to build a term-frequency matrix\n",
    "def build_term_frequency_matrix(headlines_dict):\n",
    "    all_texts = [' '.join(preprocess_text(' '.join(headlines))) for headlines in headlines_dict.values()]\n",
    "    vectorizer = CountVectorizer()\n",
    "    term_freq_matrix = vectorizer.fit_transform(all_texts)\n",
    "    return csr_matrix(term_freq_matrix), vectorizer.get_feature_names_out()\n",
    "\n",
    "# Function to calculate normalized entropy (LR) using sparse matrix operations\n",
    "def calculate_lr_matrix(term_freq_matrix, weights):\n",
    "    weighted_matrix = term_freq_matrix.T.dot(weights)\n",
    "    weighted_matrix = np.array(weighted_matrix).flatten()\n",
    "    total_weight = np.sum(weighted_matrix)\n",
    "    probabilities = weighted_matrix / total_weight\n",
    "    H_combined = -np.sum(probabilities * np.log(probabilities + 1e-10))\n",
    "    m = term_freq_matrix.shape[1]\n",
    "    LR = H_combined / np.log(m) if m > 1 else 0\n",
    "    return LR\n",
    "\n",
    "# Calculate portfolio Markowitz (Standard Deviation)\n",
    "def calculate_markowitz(returns, weights):\n",
    "    portfolio_returns = np.dot(returns, weights)\n",
    "    return np.std(portfolio_returns) * np.sqrt(252)  # Annualized volatility\n",
    "\n",
    "# Calculate portfolio VaR (DR based on VaR)\n",
    "def calculate_portfolio_var(returns, weights, confidence_level=0.95):\n",
    "    portfolio_returns = np.dot(returns, weights)\n",
    "    return np.percentile(portfolio_returns, (1 - confidence_level) * 100)\n",
    "\n",
    "# Calculate individual asset volatilities\n",
    "def calculate_individual_sds(returns):\n",
    "    return returns.std(axis=0) * np.sqrt(252)  # Annualized individual volatilities\n",
    "\n",
    "# Calculate individual asset VaRs\n",
    "def calculate_individual_vars(returns, confidence_level=0.95):\n",
    "    return returns.apply(lambda x: np.percentile(x, (1 - confidence_level) * 100))\n",
    "\n",
    "# Function to calculate DR based on Standard Deviation (dr_sd)\n",
    "def calculate_dr_sd(total_volatility, individual_volatilities, weights):\n",
    "    return total_volatility / np.sum(individual_volatilities * weights)\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def Tn(Y, Z, X=None, epsilon=1e-10):\n",
    "    n = len(Y)\n",
    "    Y = np.array([Decimal(float(y)) for y in Y])\n",
    "    X = np.array([Decimal(float(y)) for y in X])\n",
    "    X = np.array(X).reshape(-1, 1)\n",
    "    Z = np.array([Decimal(float(y)) for y in Z])\n",
    "    R = np.array([Decimal(int(rank)) for rank in np.argsort(np.argsort(Y)) + 1])\n",
    "    if X is None:\n",
    "        # Case p = 0 (no conditioning)\n",
    "        tree = cKDTree(Z)\n",
    "        _, M = tree.query(Z, k=2) # Nearest neighbor index\n",
    "        M = M[:, 1]  # Nearest neighbor index based on (X, Z)\n",
    "        L = n - R + 1  # Number of Y values >= Yi\n",
    "        numerator = np.sum(n * np.minimum(R, R[M-1]) - L ** 2)\n",
    "        denominator = np.sum(L * (n - L))\n",
    "        numerator = Decimal(numerator)\n",
    "        denominator = Decimal(denominator)\n",
    "        \n",
    "    else:\n",
    "        # Case p >= 1 (conditioning on X)\n",
    "        X_tree = cKDTree(X)\n",
    "        XZ_tree = cKDTree(np.column_stack((X, Z)))\n",
    "        \n",
    "        _, N = X_tree.query(X, k=2)\n",
    "        N = N[:, 1]  # Nearest neighbor index based on X\n",
    "        \n",
    "        _, M = XZ_tree.query(np.column_stack((X, Z)), k=2)\n",
    "        M = M[:, 1]  # Nearest neighbor index based on (X, Z)\n",
    "        numerator = max(epsilon, np.sum(np.minimum(R, R[M-1]) - np.minimum(R, R[N])))\n",
    "        denominator = max(epsilon, np.sum(R - np.minimum(R, R[N])))\n",
    "        numerator = Decimal(numerator)\n",
    "        denominator = Decimal(denominator)\n",
    "    return numerator / denominator\n",
    "\n",
    "# Conditional dependence given X\n",
    "def main(file_name, df):\n",
    "    portfolio_metrics = []\n",
    "    start_date = datetime(2018, 1, 1)\n",
    "    end_date = datetime(2024, 6, 30)\n",
    "    # Generate date ranges\n",
    "    interval_months = 12\n",
    "    date_ranges = generate_date_ranges(start_date, end_date, interval_months)\n",
    "    print(\"Processing portfolio:\", file_name)\n",
    "    \n",
    "    with open(file_name, 'r') as file:\n",
    "        tickers = [line.strip() for line in file]\n",
    "    \n",
    "    for start_dat, end_dat in date_ranges:\n",
    "        # Fetch historical returns and VIX data\n",
    "        returns, valid_tickers = fetch_historical_returns(tickers, start_dat, end_dat)\n",
    "        if not valid_tickers:\n",
    "            print(f\"No valid tickers for the period {start_dat} to {end_dat}. Skipping this period.\")\n",
    "            continue\n",
    "        \n",
    "        vix_data = fetch_vix_data(start_dat, end_dat)\n",
    "        headlines_dict = {}\n",
    "        \n",
    "        # Fetch news headlines for each valid ticker\n",
    "        for ticker in valid_tickers:\n",
    "            headlines = fetch_news_from_csv(ticker, df, start_dat, end_dat)\n",
    "            if headlines:\n",
    "                headlines_dict[ticker] = headlines\n",
    "        \n",
    "        if not headlines_dict:\n",
    "            print(f\"No headlines found for valid tickers in the period {start_dat} to {end_dat}. Skipping this period.\")\n",
    "            continue\n",
    "        \n",
    "        # Ensure that valid_tickers only includes tickers with headlines\n",
    "        valid_tickers_with_headlines = [ticker for ticker in valid_tickers if ticker in headlines_dict]\n",
    "        \n",
    "        if not valid_tickers_with_headlines:\n",
    "            print(f\"No valid tickers with headlines for the period {start_dat} to {end_dat}. Skipping this period.\")\n",
    "            continue\n",
    "        \n",
    "        # Build term frequency matrix for valid tickers with headlines\n",
    "        term_freq_matrix, _ = build_term_frequency_matrix({ticker: headlines_dict[ticker] for ticker in valid_tickers_with_headlines})\n",
    "        \n",
    "        # Use equal weights for valid tickers with headlines\n",
    "        weights = np.ones(len(valid_tickers_with_headlines)) / len(valid_tickers_with_headlines)\n",
    "        \n",
    "        # Calculate LR for the current interval\n",
    "        lr_measure = calculate_lr_matrix(term_freq_matrix, weights)\n",
    "        \n",
    "        # Calculate Markowitz (Standard Deviation) for the portfolio\n",
    "        total_volatility = calculate_markowitz(returns[valid_tickers_with_headlines], weights)\n",
    "        \n",
    "        # Calculate DR based on Standard Deviation (dr_sd)\n",
    "        individual_volatilities = calculate_individual_sds(returns[valid_tickers_with_headlines])\n",
    "        dr_sd = calculate_dr_sd(total_volatility, individual_volatilities, weights)\n",
    "        \n",
    "        # Calculate DR based on VaR (dr_var)\n",
    "        total_var = calculate_portfolio_var(returns[valid_tickers_with_headlines], weights)\n",
    "        individual_vars = calculate_individual_vars(returns[valid_tickers_with_headlines])\n",
    "        dr_var = total_var / np.sum(individual_vars * weights)\n",
    "        \n",
    "        # VIX value for the period\n",
    "        vix_value = vix_data.mean()\n",
    "        \n",
    "        # Store the portfolio metrics for this interval\n",
    "        portfolio_metrics.append((vix_value, lr_measure, total_volatility, dr_sd, dr_var))\n",
    "    \n",
    "    # Extract stored metrics\n",
    "    if portfolio_metrics:\n",
    "        vix_values, lr_measures, markowitz_measures, dr_sd_measures, dr_var_measures = zip(*portfolio_metrics)\n",
    "        \n",
    "        # Perform Azadkia-Chatterjee Method for Markowitz (SD)\n",
    "        azadkia_result_markowitz = Tn(lr_measures, markowitz_measures, vix_values)\n",
    "        print(f\"Azadkia-Chatterjee Conditional Dependence Result (Markowitz/SD): {azadkia_result_markowitz}\")\n",
    "        \n",
    "        # Perform Azadkia-Chatterjee Method for DR based on Standard Deviation (dr_sd)\n",
    "        azadkia_result_sd = Tn(lr_measures, dr_sd_measures, vix_values)\n",
    "        print(f\"Azadkia-Chatterjee Conditional Dependence Result (DR/SD): {azadkia_result_sd}\")\n",
    "        \n",
    "        # Perform Azadkia-Chatterjee Method for DR based on VaR (dr_var)\n",
    "        azadkia_result_var = Tn(lr_measures, dr_var_measures, vix_values)\n",
    "        print(f\"Azadkia-Chatterjee Conditional Dependence Result (DR/Var): {azadkia_result_var}\")\n",
    "    else:\n",
    "        print(f\"No valid portfolio metrics for {file_name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for file_name in file_array:\n",
    "        main(file_name, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d22e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e47d5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

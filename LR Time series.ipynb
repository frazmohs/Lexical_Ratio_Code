{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from numpy.random import dirichlet\n",
    "from scipy.optimize import minimize\n",
    "import yfinance as yf\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Options for optimization\n",
    "options = {\n",
    "    'maxiter': 3000,\n",
    "    'ftol': 1e-7,\n",
    "    'gtol': 1e-7,\n",
    "    'disp': False,\n",
    "    'eps': 1e-7\n",
    "}\n",
    "\n",
    "##############################################\n",
    "#         Data Loading & Global Setup        #\n",
    "##############################################\n",
    "\n",
    "# Load news data â€“ CSV must have columns: 'ticker', 'date', and 'title'\n",
    "df = pd.read_csv('news_data.csv')\n",
    "# Ensure df['date'] is in UTC\n",
    "df['date'] = pd.to_datetime(df['date']).dt.tz_convert('UTC')\n",
    "\n",
    "# Cache news data grouped by ticker for fast slicing later\n",
    "news_cache = {ticker: group.sort_values('date') for ticker, group in df.groupby('ticker')}\n",
    "\n",
    "# Define the folder where portfolio files (with tickers) are stored\n",
    "portfolio_folder = \"portfolios\"\n",
    "if os.path.exists(portfolio_folder):\n",
    "    portfolio_files = [os.path.join(portfolio_folder, file) for file in os.listdir(portfolio_folder) if file.endswith('.txt')]\n",
    "else:\n",
    "    print(f\"Portfolio folder '{portfolio_folder}' not found.\")\n",
    "    portfolio_files = []\n",
    "\n",
    "# Global cache for returns per ticker (to avoid repeated yfinance calls)\n",
    "returns_cache = {}\n",
    "\n",
    "def get_cached_returns(ticker, global_start, global_end):\n",
    "    yf_ticker = ticker.replace('.', '-')\n",
    "    if ticker not in returns_cache:\n",
    "        try:\n",
    "            data = yf.download(yf_ticker, start=global_start, end=global_end, progress=False)\n",
    "            if 'Adj Close' in data.columns:\n",
    "                data = data['Adj Close']\n",
    "            elif 'Close' in data.columns:\n",
    "                data = data['Close']\n",
    "            else:\n",
    "                return None\n",
    "            if data.empty:\n",
    "                return None\n",
    "            ret = data.pct_change().dropna()\n",
    "            returns_cache[ticker] = ret\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {ticker}: {e}\")\n",
    "            return None\n",
    "    ret = returns_cache[ticker]\n",
    "    return ret if not ret.empty else None\n",
    "\n",
    "##############################################\n",
    "#         Helper Functions                   #\n",
    "##############################################\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    return [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "\n",
    "def fetch_news_from_cache(ticker, start_date=None, end_date=None):\n",
    "    key = (ticker, start_date, end_date)\n",
    "    if key in news_slice_cache:\n",
    "        return news_slice_cache[key]\n",
    "    if ticker not in news_cache:\n",
    "        news_slice_cache[key] = []\n",
    "        return []\n",
    "    group = news_cache[ticker].copy()\n",
    "    # Ensure group dates are timezone-naive for comparison\n",
    "    group['date'] = group['date'].dt.tz_localize(None)\n",
    "    if start_date:\n",
    "        group = group[group['date'] >= pd.to_datetime(start_date)]\n",
    "    if end_date:\n",
    "        group = group[group['date'] <= pd.to_datetime(end_date)]\n",
    "    headlines = group['title'].tolist()\n",
    "    news_slice_cache[key] = headlines\n",
    "    return headlines\n",
    "\n",
    "def build_universal_vocabulary(start_date, end_date):\n",
    "    \"\"\"Build a universal vocabulary from all headlines in the entire analysis period.\"\"\"\n",
    "    # Localize start and end dates to UTC to match df['date']\n",
    "    start = pd.to_datetime(start_date).tz_localize('UTC')\n",
    "    end = pd.to_datetime(end_date).tz_localize('UTC')\n",
    "    df_filtered = df[(df['date'] >= start) & (df['date'] <= end)]\n",
    "    all_headlines = df_filtered['title'].tolist()\n",
    "    combined_text = \" \".join(all_headlines)\n",
    "    processed_words = preprocess_text(combined_text)\n",
    "    vocabulary = sorted(set(processed_words))\n",
    "    return vocabulary\n",
    "\n",
    "# Build universal vocabulary once\n",
    "global_start = '2018-01-01'\n",
    "global_end = '2024-06-30'\n",
    "news_slice_cache = {}  # cache for news slices\n",
    "universal_vocab = build_universal_vocabulary(global_start, global_end)\n",
    "print(f\"Universal vocabulary size: {len(universal_vocab)}\")\n",
    "\n",
    "def build_term_frequency_matrix(headlines_dict, vocabulary=None):\n",
    "    all_texts = [' '.join(preprocess_text(' '.join(headlines))) for headlines in headlines_dict.values()]\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    if vocabulary is not None:\n",
    "        vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer()\n",
    "    term_freq_matrix = vectorizer.fit_transform(all_texts)\n",
    "    return csr_matrix(term_freq_matrix), vectorizer.get_feature_names_out()\n",
    "\n",
    "def calculate_lr_matrix(term_freq_matrix, weights):\n",
    "    weighted_matrix = term_freq_matrix.T.dot(weights)\n",
    "    weighted_counts = np.array(weighted_matrix).flatten()\n",
    "    total_weight = np.sum(weighted_counts)\n",
    "    if total_weight == 0:\n",
    "        return np.nan\n",
    "    probabilities = weighted_counts / total_weight\n",
    "    H_combined = -np.sum(probabilities * np.log(probabilities + 1e-10))\n",
    "    m = term_freq_matrix.shape[1]  # This m is from the universal vocabulary now\n",
    "    return H_combined / np.log(m) if m > 1 else np.nan\n",
    "\n",
    "def calculate_weighted_return_percentage(returns, weights):\n",
    "    return np.sum(returns.mean() * weights) * 252\n",
    "\n",
    "def fetch_historical_returns_for_window(tickers, window_start, window_end, global_start, global_end):\n",
    "    all_returns = pd.DataFrame()\n",
    "    valid_tickers = []\n",
    "    for ticker in tickers:\n",
    "        ret = get_cached_returns(ticker, global_start, global_end)\n",
    "        if ret is not None:\n",
    "            ret_window = ret[(ret.index >= pd.to_datetime(window_start)) & (ret.index <= pd.to_datetime(window_end))]\n",
    "            if not ret_window.empty:\n",
    "                all_returns[ticker] = ret_window\n",
    "                valid_tickers.append(ticker)\n",
    "    return all_returns, valid_tickers\n",
    "\n",
    "def generate_rolling_windows(start_date, end_date, window_length_days=180, step_days=30):\n",
    "    windows = []\n",
    "    current = pd.to_datetime(start_date)\n",
    "    end_dt = pd.to_datetime(end_date)\n",
    "    while current + timedelta(days=window_length_days) <= end_dt:\n",
    "        window_start = current\n",
    "        window_end = current + timedelta(days=window_length_days)\n",
    "        windows.append((window_start.strftime('%Y-%m-%d'), window_end.strftime('%Y-%m-%d')))\n",
    "        current = current + timedelta(days=step_days)\n",
    "    return windows\n",
    "\n",
    "##############################################\n",
    "#   Optimization Function for LR Weights     #\n",
    "##############################################\n",
    "\n",
    "def optimize_portfolio_lr(tickers, returns, target_returns, headlines_dict):\n",
    "    # Use the universal vocabulary so m is fixed\n",
    "    term_freq_matrix, _ = build_term_frequency_matrix(headlines_dict, vocabulary=universal_vocab)\n",
    "    n = len(tickers)\n",
    "    results = []\n",
    "    for target_return in target_returns:\n",
    "        def objective(weights):\n",
    "            lr = calculate_lr_matrix(term_freq_matrix, weights)\n",
    "            return -lr  # maximize LR\n",
    "        constraints = [\n",
    "            {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},\n",
    "            {'type': 'ineq', 'fun': lambda w: calculate_weighted_return_percentage(returns, w) - target_return}\n",
    "        ]\n",
    "        bounds = tuple((0, 1) for _ in range(n))\n",
    "        initial_weights = np.ones(n) / n\n",
    "        result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, options=options, constraints=constraints)\n",
    "        if result.success:\n",
    "            results.append((target_return, result.x))\n",
    "    return results\n",
    "\n",
    "##############################################\n",
    "#   Plot Mean Optimized LR Time Series (Rolling Windows)  #\n",
    "##############################################\n",
    "\n",
    "# Global LR cache: key = (window_start, window_end, tuple(sorted(tickers))) ; value = computed LR\n",
    "lr_cache = {}\n",
    "\n",
    "def plot_mean_optimized_lr_time_series(portfolio_files, start_date='2018-01-01', end_date='2024-06-30', \n",
    "                                       window_length_days=180, step_days=30, target_returns=[0.07, 0.1, 0.13, 0.16]):\n",
    "    \"\"\"\n",
    "    For each portfolio file, use a rolling window approach:\n",
    "      - Each window is 180 days long and slides forward by 30 days.\n",
    "      - For each window:\n",
    "          * Fetch news headlines (from cache) for each ticker.\n",
    "          * Fetch historical returns (from cache) for these tickers.\n",
    "          * Optimize weights for multiple target returns using window data.\n",
    "          * Average the resulting weight vectors to obtain a mean weight.\n",
    "          * Build a term frequency matrix from the window's headlines using the universal vocabulary.\n",
    "          * Calculate LR using that mean weight and the fixed m.\n",
    "          * Cache LR calculations for identical windows and ticker sets.\n",
    "      - Plot the resulting LR time series using the window end date as the x-axis.\n",
    "    \"\"\"\n",
    "    windows = generate_rolling_windows(start_date, end_date, window_length_days, step_days)\n",
    "    \n",
    "    for file_name in portfolio_files:\n",
    "        with open(file_name, 'r') as f:\n",
    "            tickers = [line.strip() for line in f if line.strip()]\n",
    "        if not tickers:\n",
    "            print(f\"No tickers found in {file_name}\")\n",
    "            continue\n",
    "        \n",
    "        lr_values = []\n",
    "        time_labels = []\n",
    "        \n",
    "        for window_start, window_end in windows:\n",
    "            # Fetch headlines for each ticker in the window\n",
    "            headlines_dict = {}\n",
    "            for ticker in tickers:\n",
    "                headlines = fetch_news_from_cache(ticker, start_date=window_start, end_date=window_end)\n",
    "                headlines_dict[ticker] = headlines\n",
    "            valid_headlines_dict = {ticker: hl for ticker, hl in headlines_dict.items() if hl}\n",
    "            valid_tickers = sorted(valid_headlines_dict.keys())\n",
    "            key = (window_start, window_end, tuple(valid_tickers))\n",
    "            if key in lr_cache:\n",
    "                lr_val = lr_cache[key]\n",
    "                print(f\"Window {window_start} to {window_end}: Using cached LR value.\")\n",
    "            else:\n",
    "                if not valid_tickers:\n",
    "                    print(f\"Window {window_start} to {window_end}: No valid headlines.\")\n",
    "                    lr_val = np.nan\n",
    "                else:\n",
    "                    returns, tickers_with_returns = fetch_historical_returns_for_window(valid_tickers, window_start, window_end, global_start, global_end)\n",
    "                    if returns.empty:\n",
    "                        print(f\"Window {window_start} to {window_end}: No returns data.\")\n",
    "                        lr_val = np.nan\n",
    "                    else:\n",
    "                        opt_results = optimize_portfolio_lr(tickers_with_returns, returns, target_returns,\n",
    "                                                            {t: headlines_dict[t] for t in tickers_with_returns})\n",
    "                        if not opt_results:\n",
    "                            mean_weight = np.ones(len(tickers_with_returns)) / len(tickers_with_returns)\n",
    "                        else:\n",
    "                            weights_array = np.array([res[1] for res in opt_results])\n",
    "                            mean_weight = np.mean(weights_array, axis=0)\n",
    "                        tf_matrix, _ = build_term_frequency_matrix({t: headlines_dict[t] for t in tickers_with_returns}, vocabulary=universal_vocab)\n",
    "                        print(f\"Window {window_start} to {window_end}: TF matrix shape {tf_matrix.shape}, total count {tf_matrix.sum()}\")\n",
    "                        if tf_matrix.sum() == 0 or tf_matrix.shape[1] == 0:\n",
    "                            lr_val = np.nan\n",
    "                        else:\n",
    "                            lr_val = calculate_lr_matrix(tf_matrix, mean_weight)\n",
    "                lr_cache[key] = lr_val\n",
    "            lr_values.append(lr_val)\n",
    "            time_labels.append(datetime.strptime(window_end, '%Y-%m-%d'))\n",
    "        \n",
    "        time_index = pd.to_datetime(time_labels)\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.plot(time_index, lr_values, marker='o', linestyle='-', color='blue', label='Mean Optimized LR')\n",
    "        ax.set_xlabel('Window End Date')\n",
    "        ax.set_ylabel('Mean Optimized Lexical Ratio (LR)')\n",
    "        ax.grid(True)\n",
    "        plt.title(f'Mean Optimized LR (Rolling Windows) - {os.path.basename(file_name).replace(\".txt\", \"\")}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "# Run the rolling-window optimized LR plot for all portfolio files\n",
    "plot_mean_optimized_lr_time_series(portfolio_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fa609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba9cca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
